{"cells":[{"cell_type":"markdown","source":["<h1 align=\"center\"> Airline Delays Toy Example </h1>\n<h2 align=\"center\"> W261 - Final Project </h2>\n<h5 align=\"center\"> by Team 25: Adam Sohn, Chandra Shekar Bikkanur, Jayesh Parikh, Tucker Anderson</h5>"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"markdown","source":["### Introduction"],"metadata":{}},{"cell_type":"markdown","source":["In this example we will go through a few steps of the parallelized linear regression model. Our model sought to provide a predictive system with which flight travelers could input several features of the flight at arrival, and the model would output the predicted flight delay at the destination in minutes. In our research, we found that we achieved best regression model (by \\\\(MSE\\\\) and \\\\(R^2\\\\) through spark's mllib.regression.LinearRegression model class. This class provides an interface to train an OLS model with optional parameters (regularization, standardization etc.) based on our features."],"metadata":{}},{"cell_type":"markdown","source":["### What is parallelized multivariate OLS regression?"],"metadata":{}},{"cell_type":"markdown","source":["In Ordinary Least Squares (OLS) regression, we are attempting to minimize the squared difference between the predicted output of our model and the actual observed values of our dependent variable (in this case flight arrival delay). Here is our loss function, where \\\\(\\theta\\\\) contains our model weights (one for each variable, plus one bias term), \\\\(x_i\\\\) contains independent variable values and \\\\(\\\\) over \\\\(n\\\\) records.\n\n\\\\(f(\\theta) = \\frac{1}{n} * \\sum_{i=1}^n \\left[ \\boldsymbol{\\theta}^T\\cdot\\mathbf{x}'\\_i - y\\_i\\right]^2\\\\)\n\nAnd through the parital derivative of this loss function with respect to \\\\(\\theta\\\\):\n\n\\\\(f'(\\theta) = \\frac{2}{n}\\,\\sum\\_{i=1}^{n}\\left[ \\boldsymbol{\\theta}^T\\cdot\\mathbf{x}'\\_i - y_i\\right] \\cdot \\mathbf{x}'\\_i\\\\)\n\nAlgebraically, this gives a closed form solution to minimize our loss function (by setting the gradient to 0):\n\n\\\\(\\theta^* = (X^TX)^{-1}X^Ty\\\\)\n\nHowever, this solution is computationally intractable as \\\\(n\\\\) increases (\\\\(O(n^3)\\\\) time complexity!), and cannot be easily parallelized. The large inverted matrices and associated products cannot be served to individual partitions.\n\nIn Spark, the mllib.regression implementations use various Gradient Descent methodologies to produce parallelizable approaches to estimate the minimum gradient given our list of features and weights. In gradient descent, we descend in our model parameter space in each training cycle or epoch through the following formula:\n\n\\\\(\\theta\\_{\\text{new}} = \\theta\\_{\\text{old}} - \\eta \\cdot \\nabla\\_{\\theta} f(\\theta) \\\\)\n\nWhere \\\\(\\eta\\\\) is a configurable hyperparameter for Gradient Descent learning rate. If properly tuned, we guarantee we will converge to at least a local minimum in our parameter space.\nThis concession to the closed form solution means that we don't have a guarantee of global minimum, but we now can parallelize solutions by sending each record to isolated partitions and calculating their assocaited new gradient compenent, then finally reducing down to a total loss calculation. Each iteration of this map-reduce operation will eventually give us a minimized loss function, in much quicker time than the closed form solution."],"metadata":{}},{"cell_type":"markdown","source":["### So what's going on in our model?"],"metadata":{}},{"cell_type":"markdown","source":["Our model uses various flight features as independent variables, with \"arrival delay\" as our dependent \\\\(y\\\\) value. For this toy example, we will take a sub-section of them, namely \"DEP_DELAY\" and \"CRS_ELAPSED_TIME\". \"DEP_DELAY\" is simply the delta in minutes between the each flight was expected to depart the origin airport and when it actually left where \"CRS_ELAPSED_TIME\" represents the expected length of the flight in minutes.\n\nWe are also utilizing an elastic net regularization term (combination of L1 and L2 regularization), defined here (just a combination of L1 and L2 regression):\n\n\\\\(c = \\lambda((1-\\alpha\\sum_1^{m}\\theta^2) + (\\alpha \\sum^m_1 |\\theta|))\\\\)\n\nWhere our cross validation deemed that a value of \\\\(\\alpha\\\\) of 0.5 and a regularization term \\\\(\\lambda\\\\) of 0.1 produces the best results. Combining our gradient descent approach from above with this regularization term, we get this loss function:\n\n\\\\(f(\\theta) = \\frac{1}{n} * \\sum_{i=1}^n \\left[ \\boldsymbol{\\theta}^T\\cdot\\mathbf{x}'\\_i - y\\_i\\right]^2 + ((0.5\\sum_1^{m}\\theta^2) + (0.5\\sum^m_1 |\\theta|))\\\\)\n\nAnd taking the loss function's derivate to compute the gradient:\n\n\\\\(f'(\\theta) = \\frac{2}{n}\\,\\sum\\_{i=1}^{n}\\left[ \\boldsymbol{\\theta}^T\\cdot\\mathbf{x}'\\_i - y_i\\right] \\cdot \\mathbf{x}'\\_i + 0.5 sign(\\boldsymbol{\\theta}) + 0.5\\cdot \\theta \\\\)\n\nNow we will go through a few iterations of gradient descent, step-by-step, without any feature scaling to simplify calculations."],"metadata":{}},{"cell_type":"code","source":["FEATURE_COLUMNS = [\"INTERCEPT\", \"DEP_DELAY\", \"CRS_ELAPSED_TIME\", ]\nM = len(FEATURE_COLUMNS)\nOUTPUT_COLUMNS = [\"xj_input\", \"y_output\", \"y_predicted\", \"regularization\", \"xj_gradient\"]\ntheta = np.array([[0, 0, 0]])\nLAMBDA = 1\n\ndef gradientDesc(theta):\n\n  #100 mile flight dep delayed 0 minutes going North planned to take ten minutes\n  REC_1 = np.array([1, 0, 10])\n  #50 mile flight dep delayed 0 minutes going East planned to take 5 minutes\n  REC_2 = np.array([1, 0, 5])\n  #100 mile flight dep delayed 5 minutes going South planned to take 10 minutes\n  REC_3 = np.array([1, 5, 10])\n\n  df = pd.DataFrame([REC_1,\n                     REC_2,\n                     REC_3],\n                     columns=FEATURE_COLUMNS)\n\n  x1 = np.array([df.iloc[0]])\n  x2 = np.array([df.iloc[1]])\n  x3 = np.array([df.iloc[2]])\n\n  #flight delays for each flight\n  y1 = 10\n  y2 = 15\n  y3 = 20\n\n  #regularization term with alpha = 0.5\n  reg = ((.5*(theta)) + (.5*(np.sign(theta))))\n\n  #predicted values and calculating gradients\n  ######EACH CALCULATION CAN BE PARALLELIZED#######\n  y1_pred = np.dot(theta, x1.T)\n  loss1 = np.sum(((y1_pred - y1) * x1)**2) + .25*np.sum((theta**2)) + .5*np.sum((abs(theta)))  \n  grade1 = ((y1_pred - y1) * x1) + reg\n\n  y2_pred = np.dot(theta, x2.T)\n  loss2 = np.sum(((y2_pred - y2) * x2)**2) + .25*np.sum((theta**2)) + .5*np.sum((abs(theta)))  \n  grade2 = ((y2_pred - y2) * x2) + reg\n\n  y3_pred = np.dot(theta, x3.T)\n  loss3 = np.sum(((y1_pred - y1) * x1)**2) + .25*np.sum(.5*(theta**2)) + .5*np.sum((abs(theta)))  \n  grade3 = ((y3_pred - y3) * x3) + reg\n  \n  ######REDUCER REQUIRED HERE WHEN PARALLELIZED######\n  grade_all = (grade1 + grade2 + grade3)* (2/3)\n  loss_all = (loss1 + loss2 + loss3) /3 \n\n  #update weights with learning rate of 0.01\n  new_theta = theta - .01*(grade_all)\n  \n  return(new_theta, loss_all)\n\n# now to conduct a few gradient descents over our dataset (batch gradient descent), each time updating the weights of our model\nfor i in range(101):\n  theta,loss = gradientDesc(theta)\n  if (i%10 == 0):\n    print(\"Gradient update step\", i)\n    print(\"Loss for step\", i, \":\", loss)\n    print(\"Current weights for step\", i, theta)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Gradient update step 0\nLoss for step 0 : 8683.333333333334\nCurrent weights for step 0 [[0.3        0.66666667 2.5       ]]\nGradient update step 10\nLoss for step 10 : 1930.1072761081157\nCurrent weights for step 10 [[0.3275053  0.8301659  1.44084145]]\nGradient update step 20\nLoss for step 20 : 1773.9093994291368\nCurrent weights for step 20 [[0.459487   0.96992319 1.38639657]]\nGradient update step 30\nLoss for step 30 : 1689.1613432988604\nCurrent weights for step 30 [[0.58288983 1.02320053 1.36013942]]\nGradient update step 40\nLoss for step 40 : 1648.3242476502955\nCurrent weights for step 40 [[0.69479496 1.04490939 1.34263967]]\nGradient update step 50\nLoss for step 50 : 1623.918140578744\nCurrent weights for step 50 [[0.79529423 1.05510622 1.32911503]]\nGradient update step 60\nLoss for step 60 : 1606.3775235723504\nCurrent weights for step 60 [[0.88520927 1.06094798 1.3177879 ]]\nGradient update step 70\nLoss for step 70 : 1592.2481166344912\nCurrent weights for step 70 [[0.96553451 1.06500458 1.30794268]]\nGradient update step 80\nLoss for step 80 : 1580.2031376229809\nCurrent weights for step 80 [[1.03725014 1.06821417 1.29924983]]\nGradient update step 90\nLoss for step 90 : 1569.674607219009\nCurrent weights for step 90 [[1.10126386 1.07093276 1.29152503]]\nGradient update step 100\nLoss for step 100 : 1560.37365570617\nCurrent weights for step 100 [[1.15839746 1.07330723 1.28464272]]\n</div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["We see that our simple model weights with made up parameter values converges to the weights displayed above using our batch Gradient Descent approach.\n\nNote that this example utilizes batch Gradient Descent and not stochastic or mini-batch Gradient Descent. This means we are calculating the gradient utilizing the whole dataset at once. We chose to stick with this methodology instead of reverting to an SGD approach as our linear model converged relatively quickly utilizinig batch Gradient Descent."],"metadata":{}},{"cell_type":"markdown","source":["#### Summary\nThe hand-calculated Gradient Descent successfully reduced loss with each successive step, indicating that the process is effective for finding a set of weights that will return a minimal squared-loss value. \n\nRegarding parallelization, Spark has the opportunity to parallelize by:\n* mapping (prediction, loss, gradient) for each row to an assigned worker.\n* combining (grade_all, loss_all) is an optional step for all rows.\n* reduction (grade_all, loss_all) for all rows on a reducer."],"metadata":{}}],"metadata":{"name":"Airlines_ToyExample_Grp25","notebookId":2791835342844630},"nbformat":4,"nbformat_minor":0}
